{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from vis_det.model_pool.centernet.opts import opts\n",
    "from vis_det.model_pool.centernet.models.model import create_model, load_model, save_model\n",
    "from vis_det.model_pool.centernet.models.data_parallel import DataParallel\n",
    "from vis_det.model_pool.centernet.logger import Logger\n",
    "from vis_det.model_pool.centernet.datasets.dataset_factory import get_dataset\n",
    "from vis_det.model_pool.centernet.trains.train_factory import train_factory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix size testing.\n",
      "training chunk_sizes: [1]\n",
      "The output will be saved to  /home/paritosh/Desktop/SEM2/16824/code/vlr-project/vis_det/vis_det/model_pool/centernet/../../exp/ddd/default\n",
      "heads {'hm': 3, 'dep': 1, 'rot': 8, 'dim': 3, 'wh': 2, 'reg': 2}\n"
     ]
    }
   ],
   "source": [
    "opt = opts().parse()\n",
    "opt.debug = max(opt.debug, 1)\n",
    "torch.backends.cudnn.benchmark = not opt.not_cuda_benchmark and not opt.test\n",
    "Dataset = get_dataset(opt.dataset, opt.task)\n",
    "opt = opts().update_dataset_info_and_set_heads(opt, Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /data/datasets/paritosh/16824/centernet/ddd_3dop.pth, epoch 70\n",
      "==> initializing kitti 3dop, val data.\n",
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "Loaded val 3769 samples\n"
     ]
    }
   ],
   "source": [
    "model = create_model(opt.arch, opt.heads, opt.head_conv)\n",
    "optimizer = torch.optim.Adam(model.parameters(), opt.lr)\n",
    "model, optimizer, start_epoch = load_model(\n",
    "      model, opt.load_model, optimizer, opt.resume, opt.lr, opt.lr_step)\n",
    "Trainer = train_factory[opt.task]\n",
    "trainer = Trainer(opt, model, optimizer)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "      Dataset(opt, 'val'), \n",
    "      batch_size=1, \n",
    "      shuffle=False,\n",
    "      num_workers=1,\n",
    "      pin_memory=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> initializing kitti 3dop, train data.\n",
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "Loaded train 3712 samples\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "      Dataset(opt, 'train'), \n",
    "      batch_size=opt.batch_size, \n",
    "      shuffle=True,\n",
    "      num_workers=opt.num_workers,\n",
    "      pin_memory=True,\n",
    "      drop_last=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for iter_id, batch in enumerate(val_loader):\n",
    "    count += 1\n",
    "    #print(batch)\n",
    "    if True:\n",
    "        print(count)\n",
    "        img_ = cv2.imread(batch['meta']['image_path'][0])\n",
    "        plt.imshow(img_[:,:,::-1])\n",
    "        plt.show()\n",
    "#         initial_img = batch['input']\n",
    "#         trainer.infer(batch, torch.device(\"cuda\"), 2000)\n",
    "    if count >= 100:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invert the selected image using id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "id = 57 \n",
    "for iter_id, batch in enumerate(val_loader):\n",
    "    count += 1\n",
    "    if count==id:\n",
    "        print(count)\n",
    "        initial_img = batch['input']\n",
    "        trainer.infer(batch, torch.device(\"cuda\"), 2000)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = batch['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img,img_path = \"\"):\n",
    "    img = img.squeeze().detach().cpu()\n",
    "    img_disp = img.permute(1,2,0)\n",
    "    plt.imshow((img_disp+1)*0.5)\n",
    "    plt.imsave(img_path,((img_disp+1)*0.5).detach().cpu().numpy(), dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img(final_img, f\"inv_img_{id}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img_ = cv2.imread(batch['meta']['image_path'][0])\n",
    "img_ = cv2.resize(img_, (1280, 384))\n",
    "\n",
    "plt.imsave(f\"orig_{id}.png\",img_[:,:,::-1], dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the original Image with Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_det.model_pool.centernet.detectors.detector_factory import detector_factory\n",
    "Detector = detector_factory[opt.task]\n",
    "detector = Detector(opt)\n",
    "inp_img = batch['input'].detach().cpu()\n",
    "\n",
    "height, width = inp_img.shape[2:]\n",
    "inp_height, inp_width = opt.input_h, opt.input_w\n",
    "new_height = int(height)\n",
    "new_width  = int(width)\n",
    "c = np.array([new_width / 2., new_height / 2.], dtype=np.float32)\n",
    "s = max(height, width) * 1.0\n",
    "calib = [[721.5377197265625, 0.0, 609.559326171875, 44.85728073120117],\n",
    " [0.0, 721.5377197265625, 172.85400390625, 0.2163791060447693],\n",
    " [0.0, 0.0, 1.0, 0.0027458840049803257]]\n",
    "meta = {'c': c, 's': s, \n",
    "            'out_height': inp_height // opt.down_ratio, \n",
    "            'out_width': inp_width // opt.down_ratio,\n",
    "        'calib':np.array(calib, dtype=np.float32),\n",
    "       'img_path': f\"/home/paritosh/Desktop/SEM2/16824/code/vlr-project/vis_det/orig_{id}.png\"#batch['meta']['image_path'][0]\n",
    "       }\n",
    "img_run = {\"image\":initial_img, \"meta\":meta}\n",
    "\n",
    "detector.run(img_run, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the Inverted Image with Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_det.model_pool.centernet.detectors.detector_factory import detector_factory\n",
    "Detector = detector_factory[opt.task]\n",
    "detector = Detector(opt)\n",
    "inp_img = batch['input'].detach().cpu()\n",
    "\n",
    "height, width = inp_img.shape[2:]\n",
    "inp_height, inp_width = opt.input_h, opt.input_w\n",
    "new_height = int(height)\n",
    "new_width  = int(width)\n",
    "c = np.array([new_width / 2., new_height / 2.], dtype=np.float32)\n",
    "s = max(height, width) * 1.0\n",
    "calib = [[721.5377197265625, 0.0, 609.559326171875, 44.85728073120117],\n",
    " [0.0, 721.5377197265625, 172.85400390625, 0.2163791060447693],\n",
    " [0.0, 0.0, 1.0, 0.0027458840049803257]]\n",
    "meta = {'c': c, 's': s, \n",
    "            'out_height': inp_height // opt.down_ratio, \n",
    "            'out_width': inp_width // opt.down_ratio,\n",
    "        'calib':np.array(calib, dtype=np.float32),\n",
    "       'img_path': f\"/home/paritosh/Desktop/SEM2/16824/code/vlr-project/vis_det/inv_img_{id}.png\" #batch['meta']['image_path'][0]\n",
    "       }\n",
    "img_run = {\"image\":final_img, \"meta\":meta}\n",
    "\n",
    "detector.run(img_run, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "detectron2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
